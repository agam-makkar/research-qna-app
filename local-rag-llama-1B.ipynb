{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local RAG using Llama 3.2 1B and 3B parameter model\n",
    "Reference: \n",
    "- [Langchain RAG tutorial](#https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [Langchain document loaders][def2]\n",
    "- [Ollama library][def]\n",
    "- [Tutorial adaptive RAG from Langchain][def3]\n",
    "\n",
    "[def]: #https://ollama.com/library\n",
    "[def2]: #https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/\n",
    "[def3]: #https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary packages\n",
    "%pip install ipykernel -U --user --force-reinstall\n",
    "%pip install --quiet --upgrade langchain langchain-community\n",
    "%pip install pypdf\n",
    "%pip install --upgrade --quiet  sentence_transformers\n",
    "%pip install faiss-cpu\n",
    "%pip install --quiet -U langchain-ollama scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents that are similar to each other. It solves limitations of traditional query search engines that are optimized for hash-based searches, and provides more scalable similarity search functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data ingestion workflow\n",
    "In this part, we will first load the documents. \n",
    "- In this case we are using PyPDF loader from langchain `document_loaders` into in-memory vector store/index.\n",
    "- Second, we will chunk the documents. \n",
    "- Third, once the documents are chunked, we will use the embeddings model to generate vector embeddings. \n",
    "- Fourth, we will store the embeddings into a vector index to prepare the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF documents \n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/2401.07883v1.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "print(f\"Length of document: {len(pages)} \\n Review first page: {pages[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk documents\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "print(\"Sample chunks: \", chunks[:1])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings model\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize in-memory vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Step 1: Create in-memory vector index using HuggingFace embeddings\n",
    "faiss_index = FAISS.from_documents(chunks, hf)\n",
    "\n",
    "# Step 2: Perform similarity search to retrieve top 2 documents which are similar to the query\n",
    "# Step 2a: faiss_index.similarity_search will first embed the query using embedding model\n",
    "# Step 2b: Search the vector store to retrieve top 2 matching documents\n",
    "docs = faiss_index.similarity_search(\"Full form of RAG\", k=5)\n",
    "\n",
    "# print top 2 chunks that match the query.\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Retrieval or text generation workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, following actions will be performed. \n",
    "- First, search top 5 documents from vector index, that match the user query. \n",
    "- Second, we will create a RAG prompt which will be passed as input to the model.\n",
    "- Third, initiliaze the text generation model. \n",
    "- Fourth, augment the prompt and pass it to the model to get the answer from the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing: Store to search similar documents matching the query based on the meaning of the query.\n",
    "retriever = faiss_index.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "context = retriever.invoke(\"What is RAG?\")\n",
    "print(\"length of search results: \", len(context))\n",
    "print(\"Search results: page content\", context[0].page_content)\n",
    "print(\"Search results: metadata\", context[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "## Download llama model using ollama (Llama 3.2 is available on Ollama!)\n",
    "It's lightweight and multimodal! It's so fast and good!)\n",
    "ollama pull llama3.2:1b\n",
    "\n",
    "`ollama pull llama3.2:3b-instruct-fp16` \n",
    "\n",
    "This will download the model on your local computer. Since, we are using floating point 16 (fp16) the downloaded model will be smaller as it will use half the memory. \n",
    "We are using a model with 3B parameters which by default is stored in floating point 32(fp32). However, we are using floating point 16 which will use half the memory. The model size will approximately 6.4 GB which can easily fit on your local machine. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# model name\n",
    "local_llm_eval = \"llama3.2:3b-instruct-fp16\"\n",
    "\n",
    "local_llm = \"llama3.2:1b\"\n",
    "\n",
    "# will be used to generate responses from the model\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# will be used later while detecting hallucination in the generated response\n",
    "llm_json_mode = ChatOllama(model=local_llm_eval, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Prompt\n",
    "rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "Here is the context to use to answer the question:\n",
    "\n",
    "{context} \n",
    "\n",
    "Think carefully about the above context. \n",
    "\n",
    "Now, review the user question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Provide an answer to this questions using only the above context. \n",
    "\n",
    "Use five sentences maximum and keep the answer concise.\n",
    "If you don't know the answer simply say \"Sorry, answer not found in the context provided\".\n",
    "DO NOT use your existing knowledge, only provide answer based on the context provided. \n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "import pprint\n",
    "\n",
    "question = \"Full form of RAG?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "pprint.pp(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning sources or source attribution\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"chunk number: \", i+1)\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2\n",
    "question = \"What are the different types of chunking strategies?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "pprint.pp(generation.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(\"chunk number: \", i+1)\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import json\n",
    "def halluncination_evaluator(generation, docs_txt):\n",
    "    # Hallucination grader instructions\n",
    "    hallucination_grader_instructions = \"\"\"\n",
    "\n",
    "    You are a teacher grading a quiz. \n",
    "\n",
    "    You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "    Here is the grade criteria to follow:\n",
    "\n",
    "    (1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "\n",
    "    (2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "    Score:\n",
    "\n",
    "    A score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n",
    "\n",
    "    A score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n",
    "\n",
    "    Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "    Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "    # Grader prompt\n",
    "    hallucination_grader_prompt = \"\"\"FACTS: \\n\\n {documents} \\n\\n STUDENT ANSWER: {generation}. \n",
    "\n",
    "    Return JSON with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS. And a key, explanation, that contains an explanation of the score.\"\"\"\n",
    "\n",
    "    # Test using documents and generation from above\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(\n",
    "        documents=docs_txt, generation=generation.content\n",
    "    )\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=hallucination_grader_instructions)]\n",
    "        + [HumanMessage(content=hallucination_grader_prompt_formatted)]\n",
    "    )\n",
    "    return json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3\n",
    "question = \"What is RAG?\"\n",
    "docs = retriever.invoke(question)\n",
    "docs_txt = format_docs(docs)\n",
    "rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "\n",
    "print('------------------------------------------------')\n",
    "print('------------- Generated answer -----------------')\n",
    "print('------------------------------------------------')\n",
    "pprint.pp(generation.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('------------------------------------------------')\n",
    "print('---------- Hallucination evaluator -------------')\n",
    "print('------------------------------------------------')\n",
    "pprint.pp(halluncination_evaluator(generation, docs_txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
